Hi,

in this project, our goal is to make interactive reinforcement learning or human in the loop reinforcement learning more feasible.

The specific human-in-the-loop RL paradigm we consider in this work is interactively shaping agent behavior through binary evaluative feedback. So [you can see the middle picture in the poster], it's like there is a human observer watches the agent and the human will give positive binary feedback if the human is happy with the agent's performance. 

There are several reasons why such learning paradigm is hard to use:
1. if we are using Deep RL, we gonna need lots of human feedback, but human input is very expensive

2. secondly, such binary feedback is not that informative, it doesn't indicate why the human gives certain feedback.

So we believe one fundamental problem in human-in-the-loop RL is there is no shared vovabulary between human and the agent, and this is way the communication channel is so restrictive.

And this motivates us to try widening the communication pipeline by adding a human explanation channel. [You can see in this picture], in our setting, the human is asked to give a binary feedback and at the same time highlight task-relevant features in the image. 

The human annotation serves as a attention prior and can potentially guide the agent to learn a good representation. To make use of human attention prior, we a data augmentation technique, which only perturb irrelevant regions in image. The perturbation transformation we used is Gaussian blur. And then, after perturbing the irrelevant regions, we force the network to be invariant towards this kind of transformation.

Yeah, so another practial issue we notice is, human teachers can get exhausted if we require them to annotate every image. So we build an object-oriented interface, so that the human can explain at the level of objects rather than annotating the image all the time. 

That's basicly how this paper looks like at high level.

