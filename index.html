<!DOCTYPE HTML>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Lin Guan</title>

    <meta name="author" content="Lin Guan">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@200;400;600&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Poppins:ital,wght@1,100;1,200;1,300;1,400;1,500;1,600;1,700&display=swap"
          rel="stylesheet">

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

    <link rel="stylesheet" type="text/css" href="style.css">
    <link rel="icon" href="images/favicon.ico">
</head>

<body>
<table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
    <tr style="padding:0px">
        <td style="padding:0px">
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>

                <tr style="padding:0px">
                    <td style="padding-left:0%;padding-right:2.5%;padding-top:2.5%;width:63%;vertical-align:middle">
                        <p style="text-align:left">
                            <name>Lin Guan</name>
                            <br>
                            lguan9 [at] asu [dot] edu
                        </p>

                        <p>
                            I'm a 5th-year Ph.D. student in Computer Science at Arizona State University. I work at <a
                                href='https://yochan-lab.github.io/home/'>Yochan Lab (AI Lab)</a>, supervised by <a
                                href='https://rakaposhi.eas.asu.edu/'>Dr. Subbarao Kambhampati</a>. My research
                            interests lie at the intersection of machine learning and human-AI interaction, <span style="font-weight: bold;">especially reinforcement learning from human feedback (a.k.a. RLHF) and LLMs/LVMs for planning (a.k.a. AI agents)</span>.
                            Specifically, I am working on:
                        </p>
                        <p>
                            (a) Creation of intuitive (e.g., in concepts or natural language) interfaces that would allow the user to effortlessly provide guidance (e.g., within a human-in-the-loop RL system), specify task objectives, and steer or align the outputs of generative models.
                        </p>
                        <p>
                            (b) Development of systematic mechanisms to robustly harness planning knowledge acquired (particularly via large vision and language models) from vast yet noisy sources of collective knowledge, such as the internet.
                        </p>

                        <p>
                            <span style="font-weight: bold;color:#ff0066;">I'm on the industry and academic job market!</span>
                        </p>

                        <p style="text-align:left">
                            <a href="mailto:lguan9@asu.edu"><b>email</b></a> &nbsp&nbsp|&nbsp&nbsp
                            <a href="https://bit.ly/3sRTQGh"><b>cv</b></a> &nbsp&nbsp|&nbsp&nbsp
                            <a href="https://scholar.google.com/citations?user=c1L_gZoAAAAJ&hl=en"><b>google scholar</b></a>
                            &nbsp&nbsp|&nbsp&nbsp
                            <a href="https://twitter.com/GuanSuns"><b>twitter</b></a> &nbsp&nbsp|&nbsp&nbsp
                            <a href="https://www.linkedin.com/in/lin-guan/"><b>linkedIn</b></a> &nbsp&nbsp|&nbsp&nbsp
                            <a href="https://github.com/guansuns"><b>github</b></a>
                        </p>
                    </td>
                    <td style="padding:2.5%;width:40%;max-width:40%">
                        <a href="images/photo.jpg"><img style="width:100%;max-width:100%;border-radius:50%"
                                                        alt="profile photo" src="images/photo.jpg"></a>
                    </td>
                </tr>
                </tbody>
            </table>

            <hr>
            <!-- news feed -->
            <heading><b>News</b></heading>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding-left:2.5%;padding-bottom:20px;width:100%;vertical-align:middle">
                        <p>
                            <ul class="fa-ul">
                                <li><i class="fa fa-chevron-right"></i>
                                    <p style="display:inline;color:#ff0066;">[2023.12]</p> 1 paper accepted to <b>AAAI 2024</b>.
                                </li>
                                <li><i class="fa fa-chevron-right"></i>
                                    <p style="display:inline;color:#ff0066;">[2023.09]</p> 1 <a
                                            href='https://arxiv.org/abs/2305.14909'>paper</a> accepted to <b>NeurIPS 2023</b>.
                                </li>
                                <li><i class="fa fa-chevron-right"></i>
                                    <p style="display:inline;color:#ff0066;">[2023.01]</p> 1 <a
                                            href='https://openreview.net/forum?id=lGz9u1ubUXE'>paper</a> accepted to <b>ICLR 2023</b>.
                                </li>
                                <li><i class="fa fa-chevron-right"></i>
                                    <p style="display:inline;color:#ff0066;">[2022.05]</p> Student researcher at Google.
                                </li>
                                <li><i class="fa fa-chevron-right"></i>
                                    <p style="display:inline;color:#ff0066;">[2022.04]</p> 1 <a
                                            href='https://arxiv.org/abs/2202.02886'>paper</a> accepted to <b>ICML 2022</b> (also
                                    accepted to RLDM 2022 and received the <b>Best Paper Award</b> at <a
                                            href='https://prl-theworkshop.github.io/prl2022-icaps/'>PRL@ICAPS 2022</a>).
                                </li>
                                <li><i class="fa fa-chevron-right"></i>
                                    <p style="display:inline;color:#ff0066;">[2021.10]</p> 1 <a
                                            href='https://arxiv.org/abs/2006.14804'>paper</a> accepted to <b>NeurIPS 2021</b> as
                                    a <b>Spotlight</b> presentation (3%).
                                </li>
                                <li><i class="fa fa-chevron-right"></i>
                                    <p style="display:inline;color:#ff0066;">[2021.05]</p> Machine learning software engineer
                                    intern at <a href='https://www.tiktok.com/'>TikTok</a> and worked on AutoML.
                                </li>
                            </ul>

                            <!-- OLDER NEWS -->
                            <ul class="fa-ul">
                                <p style="color:black;">
                                    <button class="button1" onclick="myFunction()">
                                <p style="color:black;"><i class="fa fa-chevron-down" aria-hidden="true"></i> [Older News]
                                </p></button></p>

                                <div id="last_name_button" style="display:none;">
                                    <li><i class="fa fa-chevron-right" aria-hidden="true"></i>
                                        <p style="display:inline;color:#ff0066;">[2021.10]</p> 1 <a
                                                href='https://arxiv.org/abs/2109.09904'>paper</a> accepted to <b>AAAI 2022</b> Blue
                                        Sky Track.
                                    </li>
                                    <li><i class="fa fa-chevron-right" aria-hidden="true"></i>
                                        <p style="display:inline;color:#ff0066;">[2021.06]</p> 1 <a
                                                href='https://arxiv.org/abs/2104.00878'>paper</a> accepted to <b>IROS 2021</b>.
                                    </li>
                                    <li><i class="fa fa-chevron-right" aria-hidden="true"></i>
                                        <p style="display:inline;color:#ff0066;">[2019.12]</p> 1 <a
                                                href='https://arxiv.org/abs/1903.06754'>paper</a> accepted to <b>AAAI
                                            2020</b>.
                                    </li>
                                    <li><i class="fa fa-chevron-right" aria-hidden="true"></i>
                                        <p style="display:inline;color:#ff0066;">[2019.04]</p> 1 <a
                                                href='https://arxiv.org/abs/1909.09906'>paper</a> accepted to <b>IJCAI
                                            2019</b>.
                                    </li>
                                </div>

                                <script>
                                    function myFunction() {
                                        var x = document.getElementById("last_name_button");
                                        if (x.style.display === "none") {
                                            x.style.display = "block";
                                        } else {
                                            x.style.display = "none";
                                        }
                                    }
                                </script>
                            </ul>
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>

            <!-- Affiliations -->
            <hr>
            <heading><b>Affiliations</b></heading>
            <br><br>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tbody>
                    <td style="padding-left:20px;padding-bottom:5px;vertical-align:middle;text-align:center;width:15%;">
                        <img src="images/cruise.png" style="width:42px">
                    </td>
                    <td style="padding-left:2px;padding-bottom:5px;width:55%;vertical-align:left">
                        <b>Cruise</b>, PhD Intern, Planning ML
                    </td>
                    <td style="padding-bottom:5px;width:30%;vertical-align:right">
                        Summer 2023
                    </td>
                    </tr>
                    </tbody>
                </table>

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tbody>
                    <td style="padding-left:20px;padding-bottom:1px;vertical-align:middle;text-align:center;width:15%;">
                        <img src="images/google.png" style="width:25px">
                    </td>
                    <td style="padding-left:2px;padding-bottom:5px;width:55%;vertical-align:left">
                        <b>Google</b>, Research Intern
                    </td>
                    <td style="padding-bottom:5px;width:30%;vertical-align:right">
                        Summer 2022
                    </td>
                    </tr>
                    </tbody>
                </table>

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tbody>
                    <td style="padding-left:20px;padding-bottom:2px;vertical-align:middle;text-align:center;width:15%;">
                        <img src="images/tiktok.png" style="width:25px">
                    </td>
                    <td style="padding-left:2px;padding-bottom:5px;width:55%;vertical-align:left">
                        <b>TikTok</b>, Machine Learning Software Engineer Intern
                    </td>
                    <td style="padding-bottom:5px;width:30%;vertical-align:right">
                        Summer 2021
                    </td>
                    </tr>
                    </tbody>
                </table>

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tbody>
                    <td style="padding-left:20px;padding-bottom:2px;vertical-align:middle;text-align:center;width:15%;">
                        <img src="images/asu.png" style="width:42px">
                    </td>
                    <td style="padding-left:2px;padding-bottom:5px;width:55%;vertical-align:left">
                        <b>Arizona State University</b>, Ph.D. in Computer Science
                    </td>
                    <td style="padding-bottom:5px;width:30%;vertical-align:right">
                        Fall 2019 - Expected Spring 2024
                    </td>
                    </tr>
                    </tbody>
                </table>

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tbody>
                    <td style="padding-left:20px;padding-bottom:2px;vertical-align:middle;text-align:center;width:15%;">
                        <img src="images/ut_austin.png" style="width:70px">
                    </td>
                    <td style="padding-left:2px;padding-bottom:8px;width:55%;vertical-align:left">
                        <b>The University of Texas at Austin</b>, B.S. in Computer Science
                    </td>
                    <td style="padding-bottom:8px;width:30%;vertical-align:right">
                        Fall 2016 - Spring 2019
                    </td>
                    </tr>
                    </tbody>
                </table>


                <p></p>
                </tbody>
            </table>
        </td>
    </tr>
    </tbody>
</table>

<table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
    <tr style="padding:0px">
        <td style="padding:0px">
            <!-- Publications -->
            <hr>
            <heading><b>Selected Publications</b></heading>
            <br>
            <br>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tbody>
                    <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle"><img
                            src="images/paper_imgs/2023_llm_dm.png" style="border-radius:5%/10%;width:200px"></td>
                    <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
                        <a href="https://arxiv.org/abs/2305.14909">
                            <papertitle>
                                Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning
                            </papertitle>
                        </a>
                        <br>
                        <u>Lin Guan</u>*, Karthik Valmeekam*, Sarath Sreedharan, Subbarao Kambhampati
                        <br>
                        <conftitle>NeurIPS 2023</conftitle>
                        <br>
                        <p style="color:#4E4E4E;">To leverage LLMs in planning tasks, we introduce an alternative paradigm that teases an explicit world (domain) model in PDDL out of LLMs and then uses it to plan with sound domain-independent planners. This is motivated by the insight that LLMs, while incapable of the combinatorial search needed to produce correct plans, may be better suited as the source of world models.</p>
                        <a class="button" href="https://arxiv.org/abs/2305.14909"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
                        &nbsp
                        <a class="button-website" href="./pages/llm-dm/index.html"><i class="fa fa-external-link" aria-hidden="true"></i> website</a>
                        <p></p>
                    </td>
                    </tr>
                    </tbody>
                </table>

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tbody>
                    <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle"><img
                            src="images/paper_imgs/2023_llm_tutorial.png" style="border-radius:5%/10%;width:200px"></td>
                    <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
                        <a href="https://icaps23.icaps-conference.org/program/tutorials/llms/">
                            <papertitle>
                                On the role of Large Language Models in Planning
                            </papertitle>
                        </a>
                        <br>
                        Subbarao Kambhampati, Karthik Valmeekam, Matthew Marquez, <u>Lin Guan</u>
                        <br>
                        <conftitle>Tutorial at ICAPS 2023</conftitle>
                        (also accepted to 
                        <conftitle>AAAI 2024 Tutorial Program</conftitle>)
                        <br>
                        <p style="color:#4E4E4E;">This tutorial discusses the fundamental limitations of LLMs in generating plans (especially those that require resolving subgoal interactions), and also presents constructive uses of LLMs for planning tasks.</p>
                        <a class="button-website" href="https://icaps23.icaps-conference.org/program/tutorials/llms/"><i class="fa fa-external-link" aria-hidden="true"></i> website</a>
                        <p></p>
                    </td>
                    </tr>
                    </tbody>
                </table>

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tbody>
                    <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle"><img
                            src="images/paper_imgs/2023_rel_attr.gif" style="border-radius:5%/10%;width:200px"></td>
                    <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
                        <a href="https://arxiv.org/abs/2210.15906">
                            <papertitle>
                                Relative Behavioral Attributes: Filling the Gap between Symbolic Goal Specification and Reward Learning from Human Preferences
                            </papertitle>
                        </a>
                        <br>
                        <u>Lin Guan</u>, Karthik Valmeekam, Subbarao Kambhampati
                        <br>
                        <conftitle>ICLR 2023</conftitle>
                        <br>
                        <p style="color:#4E4E4E;">We introduce the notion of Relative Behavioral Attributes that
                            enables end users to tweak the agent's behavior through nameable concepts even for a tacit-knowledge skill learning task
                            (e.g., decreasing the <b>steering sharpness</b> of an autonomous driving agent,
                            or increasing the <b>softness of the movement</b> of a two-legged "sneaky" agent).</p>
                        <a class="button" href="https://arxiv.org/abs/2210.15906"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
                        &nbsp
                        <a class="button-website" href="./pages/rba/index.html"><i class="fa fa-external-link" aria-hidden="true"></i> website</a>
                        <p></p>
                    </td>
                    </tr>
                    </tbody>
                </table>

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tbody>
                    <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle"><img
                            src="images/paper_imgs/2021_expand.png" style="border-radius:5%/10%;width:200px"></td>
                    <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
                        <a href="https://arxiv.org/abs/2006.14804">
                            <papertitle>Widening the Pipeline in Human-Guided Reinforcement Learning with Explanation
                                and Context-Aware Data Augmentation
                            </papertitle>
                        </a>
                        <br>
                        <u>Lin Guan</u>, Mudit Verma, Sihang Guo, Ruohan Zhang, Subbarao Kambhampati
                        <br>
                        <conftitle>NeurIPS 2021 (Spotlight, 3%)</conftitle>
                        <br>
                        <p style="color:#4E4E4E;">We make Human-in-the-Loop RL more efficient and feasible by allowing human teachers to
                            not only give binary feedback but also to highlight task-relevant features.</p>
                        <a class="button" href="https://arxiv.org/abs/2006.14804"><i class="fa fa-file-text"
                                                                                     aria-hidden="true"></i> paper</a>
                        <p></p>
                    </td>
                    </tr>
                    </tbody>
                </table>

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tbody>
                    <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle"><img
                            src="images/paper_imgs/2022_asgrl.gif" style="border-radius:5%/10%;width:200px"></td>
                    <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
                        <a href="https://arxiv.org/abs/2202.02886">
                            <papertitle>Leveraging Approximate Symbolic Models for Reinforcement Learning via Skill Diversity
                            </papertitle>
                        </a>
                        <br>
                        <u>Lin Guan</u>*, Sarath Sreedharan* (equal contribution), Subbarao Kambhampati
                        <br>
                        <conftitle>ICML 2022</conftitle>
                        (also received the
                        <conftitle>Best Paper Award</conftitle>
                        at PRL@ICAPS 2022 and accepted to RLDM 2022)
                        <br>
                        <p style="color:#4E4E4E;">Explicit symbolic knowledge is important for solving long-horizon task and motion planning tasks.
                            But a key resistance to leveraging easily available human knowledge (or knowledge acquired from LLMs/VLMs) has been that it might be <b>inexact</b>.
                            In this work, we present a framework to quantify the relationship between the true task model and an inexact STRIPS model, and
                            introduce a novel approach using landmarks and a diversity objective to make up for potential errors in the symbolic knowledge. </p>
                        <a class="button" href="https://arxiv.org/abs/2202.02886"><i class="fa fa-file-text"
                                                                                     aria-hidden="true"></i> paper</a>
                        &nbsp
                        <a class="button-website" href="./pages/asgrl/index.html"><i class="fa fa-external-link" aria-hidden="true"></i> website</a>
                        <p></p>
                    </td>
                    </tr>
                    </tbody>
                </table>

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tbody>
                    <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle"><img
                            src="images/paper_imgs/2019_survey.png" style="border-radius:5%/10%;width:200px"></td>
                    <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
                        <a href="https://arxiv.org/abs/1909.09906">
                            <papertitle>Leveraging Human Guidance for Deep Reinforcement Learning Tasks</papertitle>
                        </a>
                        <br>
                        Ruohan Zhang, Faraz Torabi, <u>Lin Guan</u>, Dana H. Ballard, Peter Stone
                        <br>
                        <conftitle>IJCAI 2019, Survey Track</conftitle>
                        <br>
                        <p style="color:#4E4E4E;">
                            This survey provides a high-level overview of five learning frameworks that
                            primarily rely on human guidance other than conventional, step-by-step action demonstrations.
                        </p>
                        <a class="button" href="https://arxiv.org/abs/1909.09906"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
                    </td>
                    </tr>
                    </tbody>
                </table>

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tbody>
                    <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle">
                        <img src="images/paper_imgs/2020_self_explain.png" style="border-radius:5%/10%;width:200px">
                    </td>
                    <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
                        <a href="https://arxiv.org/abs/2110.05286">
                            <papertitle>Learning from Ambiguous Demonstrations with Self-Explanation Guided Reinforcement Learning
                            </papertitle>
                        </a>
                        <br>
                        Yantian Zha, <u>Lin Guan</u>, Subbarao Kambhampati
                        <br>
                        <conftitle>AAAI 2024</conftitle>
                        <br>
                        <p style="color:#4E4E4E;">
                            We showcase how explicit characterization of a domain can facilitate reinforcement learning from ambiguous demonstrations.
                        </p>
                        <a class="button" href="https://arxiv.org/abs/2110.05286"><i class="fa fa-file-text"
                                                                                     aria-hidden="true"></i> paper</a>
                        <p></p>
                    </td>
                    </tr>
                    </tbody>
                </table>

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tbody>
                    <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle"><img
                            src="images/paper_imgs/2021_lingua_franca.png" style="border-radius:5%/10%;width:200px"></td>
                    <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
                        <a href="https://arxiv.org/abs/2109.09904">
                            <papertitle>Symbols as a Lingua Franca for Bridging Human-AI Chasm for Explainable and
                                Advisable AI Systems
                            </papertitle>
                        </a>
                        <br>
                        Subbarao Kambhampati, Sarath Sreedharan, Mudit Verma, Yantian Zha, <u>Lin Guan</u>
                        <br>
                        <conftitle>AAAI 2022, Blue Sky Track</conftitle>
                        <br>
                        <p style="color:#4E4E4E;">
                            We advocated an ambitious research program
                            to broaden the basis for human-AI interaction by proposing
                            that AI systems support a symbolic interface – independent
                            of whether their internal operations themselves are done in
                            human-interpretable symbolic means.
                        </p>
                        <a class="button" href="https://arxiv.org/abs/2109.09904"><i class="fa fa-file-text"
                                                                                     aria-hidden="true"></i> paper</a>
                    </td>
                    </tr>
                    </tbody>
                </table>

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tbody>
                    <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle"><img
                            src="images/paper_imgs/2021_iros.png" style="border-radius:5%/10%;width:200px"></td>
                    <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
                        <a href="https://arxiv.org/abs/2104.00878">
                            <papertitle>
                                Contrastively Learning Visual Attention as Affordance Cues from Demonstrations
                                for Robotic Grasping
                            </papertitle>
                        </a>
                        <br>
                        Yantian Zha, Siddhant Bhambri, <u>Lin Guan</u>
                        <br>
                        <conftitle>IROS 2021</conftitle>
                        <br>
                        <p></p>
                        <a class="button" href="https://arxiv.org/abs/2104.00878"><i class="fa fa-file-text"
                                                                                     aria-hidden="true"></i> paper</a>
                        <p></p>
                    </td>
                    </tr>
                    </tbody>
                </table>

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tbody>
                    <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle"><img
                            src="images/paper_imgs/2019_atari_head.png" style="border-radius:5%/10%;width:200px"></td>
                    <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
                        <a href="https://arxiv.org/abs/1903.06754">
                            <papertitle>Atari-HEAD: Atari Human Eye-Tracking and Demonstration Dataset</papertitle>
                        </a>
                        <br>
                        Ruohan Zhang, Calen Walshe, Zhuode Liu, <u>Lin Guan</u>, Karl S. Muller, Jake A. Whritner, Luxin
                        Zhang, Mary M Hayhoe, Dana H Ballard
                        <br>
                        <conftitle>AAAI 2020</conftitle>
                        <br>
                        <p style="color:#4E4E4E;">
                            We provide a large-scale, high-quality dataset of human actions with simultaneously
                            recorded eye movements (i.e., gaze info) while humans play Atari video games.
                        </p>
                        <a class="button" href="https://arxiv.org/abs/1903.06754"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
                    </td>
                    </tr>
                    </tbody>
                </table>

                </tbody>
            </table>
        </td>
    </tr>
    </tbody>
</table>

<table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
    <tr style="padding:0px">
        <td style="padding:0px">

            <!-- Service  -->
            <hr>
            <heading><b>Service</b></heading>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding-left:2.5%;padding-bottom:20px;width:100%;vertical-align:middle">

                        <p><ul class="fa-ul">
                            <li><i class="fa fa-chevron-right"></i>
                                <p style="display:inline;color:#ff0066;">[2023]</p> Served as a reviewer for ICML 2023 and NeurIPS 2023.
                            </li>
                            <li><i class="fa fa-chevron-right"></i>
                                <p style="display:inline;color:#ff0066;">[2022]</p> Served as a reviewer for NeurIPS 2022.
                            </li>
                            <li><i class="fa fa-chevron-right"></i>
                                <p style="display:inline;color:#ff0066;">[2021]</p> Served as program committee for AAAI 2022 and ICRA 2022.
                            </li>
                            <li><i class="fa fa-chevron-right"></i>
                                <p style="display:inline;color:#ff0066;">[2020]</p> Served as program committee for AAAI 2021.
                            </li>
                        </ul>
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>

            <hr>
            <p align="center" style="font-size:small;color:grey;">
                website adapted from <a href="https://people.eecs.berkeley.edu/~abajcsy"
                                        style="font-size:small;">here</a>
                </font>
            </p>
        </td>
    </tr>
    </tbody>
</table>


<script async src="//static.getclicky.com/101386582.js"></script>
<noscript><p><img alt="Clicky" width="1" height="1" src="//in.getclicky.com/101386582ns.gif" /></p></noscript>
<script src="https://www.w3counter.com/tracker.js?id=147571"></script>


</body>
</html>