<!DOCTYPE HTML>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Lin Guan</title>

    <meta name="author" content="Lin Guan">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@200;400;600&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Poppins:ital,wght@1,100;1,200;1,300;1,400;1,500;1,600;1,700&display=swap"
          rel="stylesheet">

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

    <link rel="stylesheet" type="text/css" href="style.css">
    <link rel="icon" href="images/favicon.ico">
</head>

<body>
<table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
    <tr style="padding:0px">
        <td style="padding:0px">
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>

                <tr style="padding:0px">
                    <td style="padding-left:0%;padding-right:2.5%;padding-top:2.5%;width:63%;vertical-align:middle">
                        <p style="text-align:left">
                            <name>Lin Guan</name>
                            <br>
                            lguan9 [at] asu [dot] edu
                        </p>

                        <p>
                            I'm a 4th-year Ph.D. student in Computer Science at Arizona State University. I work at <a
                                href='https://yochan-lab.github.io/home/'>Yochan Lab (AI Lab)</a>, supervised by <a
                                href='https://rakaposhi.eas.asu.edu/'>Dr. Subbarao Kambhampati</a>. My research
                            interests lie at the intersection of <span style="font-weight: bold;">machine learning (especially reinforcement learning and computer vision), robotics, and human-robot interaction</span>.
                            Specifically, I am working on:
                        </p>
                        <p>
                            (a) Developing algorithms that enable human users to effortlessly specify desired robot
                            behaviors through human inputs such as symbolic concepts, natural language feedback,
                            preference labels, demonstrations, and human gaze/attention information, etc. Applications
                            include character/animation control, autonomous vehicles, and robot skill learning.
                        </p>
                        <p>
                            (b) Building intelligent agents that can learn to solve complex long-horizon tasks by
                            integrating planning with reinforcement learning, and leveraging symbolic human knowledge
                            (e.g., in the form of incomplete/imprecise symbolic domain models).
                        </p>

                        <p style="text-align:left">
                            <a href="mailto:lguan9@asu.edu"><b>email</b></a> &nbsp&nbsp|&nbsp&nbsp
                            <a href="https://bit.ly/3sRTQGh"><b>cv</b></a> &nbsp&nbsp|&nbsp&nbsp
                            <a href="https://scholar.google.com/citations?user=c1L_gZoAAAAJ&hl=en"><b>google scholar</b></a>
                            &nbsp&nbsp|&nbsp&nbsp
                            <a href="https://twitter.com/GuanSuns"><b>twitter</b></a> &nbsp&nbsp|&nbsp&nbsp
                            <a href="https://www.linkedin.com/in/lin-guan/"><b>linkedIn</b></a> &nbsp&nbsp|&nbsp&nbsp
                            <a href="https://github.com/guansuns"><b>github</b></a>
                        </p>
                    </td>
                    <td style="padding:2.5%;width:40%;max-width:40%">
                        <a href="images/photo.jpg"><img style="width:100%;max-width:100%;border-radius:50%"
                                                        alt="profile photo" src="images/photo.jpg"></a>
                    </td>
                </tr>
                </tbody>
            </table>

            <hr>
            <!-- news feed -->
            <heading><b>News</b></heading>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding-left:2.5%;padding-bottom:20px;width:100%;vertical-align:middle">
                        <p><ul class="fa-ul">
                            <li><i class="fa fa-chevron-right"></i>
                        <p style="display:inline;color:#ff0066;">[2022.05]</p> Student researcher at Google.
                        </li>
                        <li><i class="fa fa-chevron-right"></i>
                            <p style="display:inline;color:#ff0066;">[2022.04]</p> 1 <a
                                    href='https://arxiv.org/abs/2202.02886'>paper</a> accepted to <b>ICML 2022</b> (also
                            accepted to RLDM 2022 and received the <b>Best Paper Award</b> at <a
                                    href='https://prl-theworkshop.github.io/prl2022-icaps/'>PRL@ICAPS 2022</a>).
                        </li>
                        <li><i class="fa fa-chevron-right"></i>
                            <p style="display:inline;color:#ff0066;">[2021.12]</p> 1 <a
                                    href='https://arxiv.org/abs/2112.03487'>paper</a> accepted to AAAI-22 Workshop on
                            Practical Deep Learning in the Wild.
                        </li>
                        <li><i class="fa fa-chevron-right"></i>
                            <p style="display:inline;color:#ff0066;">[2021.12]</p> 1 <a
                                    href='https://arxiv.org/abs/2110.05286'>paper</a> accepted to AAAI-22 Workshop on
                            Reinforcement Learning in Games.
                        </li>
                        <li><i class="fa fa-chevron-right"></i>
                            <p style="display:inline;color:#ff0066;">[2021.10]</p> 1 <a
                                    href='https://arxiv.org/abs/2006.14804'>paper</a> accepted to <b>NeurIPS 2021</b> as
                            a <b>Spotlight</b> presentation (3%).
                        </li>
                        <li><i class="fa fa-chevron-right"></i>
                            <p style="display:inline;color:#ff0066;">[2021.10]</p> 1 <a
                                    href='https://arxiv.org/abs/2109.09904'>paper</a> accepted to <b>AAAI 2022</b> Blue
                            Sky Track.
                        </li>
                        <li><i class="fa fa-chevron-right"></i>
                            <p style="display:inline;color:#ff0066;">[2021.06]</p> 1 <a
                                    href='https://arxiv.org/abs/2104.00878'>paper</a> accepted to <b>IROS 2021</b>.
                        </li>
                        <li><i class="fa fa-chevron-right"></i>
                            <p style="display:inline;color:#ff0066;">[2021.05]</p> Machine learning software engineer
                            intern at <a href='https://www.tiktok.com/'>TikTok</a> and worked on AutoML.
                        </li>
                        <li><i class="fa fa-chevron-right"></i>
                            <p style="display:inline;color:#ff0066;">[2020.12]</p> Participated in the Deep
                            Reinforcement Learning workshop at NeurIPS 2020.
                        </li>
                        </ul>

                        <!-- OLDER NEWS -->
                        <ul class="fa-ul">
                            <p style="color:black;">
                                <button class="button1" onclick="myFunction()">
                            <p style="color:black;"><i class="fa fa-chevron-down" aria-hidden="true"></i> [Older News]
                            </p></button></p>

                            <div id="last_name_button" style="display:none;">
                                <li><i class="fa fa-chevron-right" aria-hidden="true"></i>
                                    <p style="display:inline;color:#ff0066;">[2020.07]</p> Participated in the
                                    Human-in-the-Loop Learning workshop at ICML 2020.
                                </li>
                                <li><i class="fa fa-chevron-right" aria-hidden="true"></i>
                                    <p style="display:inline;color:#ff0066;">[2019.12]</p> 1 <a
                                            href='https://arxiv.org/abs/1903.06754'>paper</a> accepted to <b>AAAI
                                        2020</b>.
                                </li>
                                <li><i class="fa fa-chevron-right" aria-hidden="true"></i>
                                    <p style="display:inline;color:#ff0066;">[2019.04]</p> 1 <a
                                            href='https://arxiv.org/abs/1909.09906'>paper</a> accepted to <b>IJCAI
                                        2019</b>.
                                </li>
                            </div>

                            <script>
                                function myFunction() {
                                    var x = document.getElementById("last_name_button");
                                    if (x.style.display === "none") {
                                        x.style.display = "block";
                                    } else {
                                        x.style.display = "none";
                                    }
                                }
                            </script>
                        </ul>
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>

            <!-- Affiliations -->
            <hr>
            <heading><b>Affiliations</b></heading>
            <br><br>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tbody>
                    <td style="padding-left:20px;padding-bottom:1px;vertical-align:middle;text-align:center;width:15%;">
                        <img src="images/google.png" style="width:25px">
                    </td>
                    <td style="padding-left:2px;padding-bottom:5px;width:55%;vertical-align:left">
                        <b>Google</b>, Research Intern
                    </td>
                    <td style="padding-bottom:5px;width:30%;vertical-align:right">
                        Summer 2022
                    </td>
                    </tr>
                    </tbody>
                </table>

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tbody>
                    <td style="padding-left:20px;padding-bottom:2px;vertical-align:middle;text-align:center;width:15%;">
                        <img src="images/tiktok.png" style="width:25px">
                    </td>
                    <td style="padding-left:2px;padding-bottom:5px;width:55%;vertical-align:left">
                        <b>TikTok</b>, Machine Learning Software Engineer Intern
                    </td>
                    <td style="padding-bottom:5px;width:30%;vertical-align:right">
                        Summer 2021
                    </td>
                    </tr>
                    </tbody>
                </table>

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tbody>
                    <td style="padding-left:20px;padding-bottom:2px;vertical-align:middle;text-align:center;width:15%;">
                        <img src="images/asu.png" style="width:42px">
                    </td>
                    <td style="padding-left:2px;padding-bottom:5px;width:55%;vertical-align:left">
                        <b>Arizona State University</b>, Ph.D. in Computer Science
                    </td>
                    <td style="padding-bottom:5px;width:30%;vertical-align:right">
                        Fall 2019 - Expected Spring 2024
                    </td>
                    </tr>
                    </tbody>
                </table>

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tbody>
                    <td style="padding-left:20px;padding-bottom:2px;vertical-align:middle;text-align:center;width:15%;">
                        <img src="images/ut_austin.png" style="width:70px">
                    </td>
                    <td style="padding-left:2px;padding-bottom:8px;width:55%;vertical-align:left">
                        <b>The University of Texas at Austin</b>, B.S. in Computer Science
                    </td>
                    <td style="padding-bottom:8px;width:30%;vertical-align:right">
                        Fall 2016 - Spring 2019
                    </td>
                    </tr>
                    </tbody>
                </table>


                <p></p>
                </tbody>
            </table>
        </td>
    </tr>
    </tbody>
</table>

<table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
    <tr style="padding:0px">
        <td style="padding:0px">
            <!-- Publications -->
            <hr>
            <heading><b>Selected Publications</b></heading>
            <br>
            <br>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tbody>
                    <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle"><img
                            src="images/paper_imgs/2023_rel_attr.gif" style="border-radius:5%/10%;width:200px"></td>
                    <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
                        <a href="https://bit.ly/3f8mvTZ">
                            <papertitle>
                                Relative Behavioral Attributes: Filling the Gap between Symbolic Goal Specification and Reward Learning from Human Preferences
                            </papertitle>
                        </a>
                        <br>
                        <u>Lin Guan</u>, Karthik Valmeekam, Subbarao Kambhampati
                        <br>
                        <conftitle>NeurIPS 2022 HiLL</conftitle>
                        <br>
                        <p style="color:#4E4E4E;">We introduce the notion of Relative Behavioral Attributes that
                            enables end users to tweak the agent's behavior through nameable concepts even for a tacit-knowledge skill learning task
                            (e.g., decreasing the <b>steering sharpness</b> of an autonomous driving agent,
                            or increasing the <b>softness of the movement</b> of a two-legged "sneaky" agent).</p>
                        <a class="button" href="https://bit.ly/3f8mvTZ"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
                        <p></p>
                    </td>
                    </tr>
                    </tbody>
                </table>

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tbody>
                    <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle"><img
                            src="images/paper_imgs/2022_asgrl.gif" style="border-radius:5%/10%;width:200px"></td>
                    <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
                        <a href="https://arxiv.org/abs/2202.02886">
                            <papertitle>Leveraging Approximate Symbolic Models for Reinforcement Learning via Skill
                                Diversity
                            </papertitle>
                        </a>
                        <br>
                        <u>Lin Guan</u>*, Sarath Sreedharan* (equal contribution), Subbarao Kambhampati
                        <br>
                        <conftitle>ICML 2022</conftitle>
                        (also received the
                        <conftitle>Best Paper Award</conftitle>
                        at PRL@ICAPS 2022 and accepted to RLDM 2022)
                        <br>
                        <p style="color:#4E4E4E;">Symbolic knowledge is important for solving long-horizon task and motion planning tasks.
                            But a key resistance to leveraging easily available human symbolic knowledge has been that it might be <b>inexact</b>.
                            In this work, we present a framework to quantify the relationship between the true task model and an inexact STRIPS model, and
                            introduce a novel approach using landmarks and a diversity objective to make up for potential errors in the symbolic knowledge. </p>
                        <a class="button" href="https://arxiv.org/abs/2202.02886"><i class="fa fa-file-text"
                                                                                     aria-hidden="true"></i> paper</a>
                        <p></p>
                    </td>
                    </tr>
                    </tbody>
                </table>

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tbody>
                    <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle"><img
                            src="images/paper_imgs/2021_expand.png" style="border-radius:5%/10%;width:200px"></td>
                    <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
                        <a href="https://arxiv.org/abs/2006.14804">
                            <papertitle>Widening the Pipeline in Human-Guided Reinforcement Learning with Explanation
                                and Context-Aware Data Augmentation
                            </papertitle>
                        </a>
                        <br>
                        <u>Lin Guan</u>, Mudit Verma, Sihang Guo, Ruohan Zhang, Subbarao Kambhampati
                        <br>
                        <conftitle>NeurIPS 2021 (Spotlight, 3%)</conftitle>
                        <br>
                        <p style="color:#4E4E4E;">We make Human-in-the-Loop RL more efficient and feasible by allowing human teachers to
                            not only give binary feedback but also to highlight task-relevant features.</p>
                        <a class="button" href="https://arxiv.org/abs/2006.14804"><i class="fa fa-file-text"
                                                                                     aria-hidden="true"></i> paper</a>
                        <p></p>
                    </td>
                    </tr>
                    </tbody>
                </table>

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tbody>
                    <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle"><img
                            src="images/paper_imgs/2021_lingua_franca.png" style="border-radius:5%/10%;width:200px"></td>
                    <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
                        <a href="https://arxiv.org/abs/2109.09904">
                            <papertitle>Symbols as a Lingua Franca for Bridging Human-AI Chasm for Explainable and
                                Advisable AI Systems
                            </papertitle>
                        </a>
                        <br>
                        Subbarao Kambhampati, Sarath Sreedharan, Mudit Verma, Yantian Zha, <u>Lin Guan</u>
                        <br>
                        <conftitle>AAAI 2022, Blue Sky Track</conftitle>
                        <br>
                        <a class="button" href="https://arxiv.org/abs/2109.09904"><i class="fa fa-file-text"
                                                                                     aria-hidden="true"></i> paper</a>
                        <p style="color:#4E4E4E;">
                            We advocated an ambitious research program
                            to broaden the basis for human-AI interaction by proposing
                            that AI systems support a symbolic interface â€“ independent
                            of whether their internal operations themselves are done in
                            human-interpretable symbolic means.
                        </p>
                    </td>
                    </tr>
                    </tbody>
                </table>

                <!--
                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tbody>
                    <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle">
                        <img src="images/paper_imgs/2020_self_explain.png" style="border-radius:5%/10%;width:200px">
                    </td>
                    <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
                        <a href="https://arxiv.org/abs/2110.05286">
                            <papertitle>Learning from Ambiguous Demonstrations with Self-Explanation Guided
                                Reinforcement Learning
                            </papertitle>
                        </a>
                        <br>
                        Yantian Zha, <u>Lin Guan</u>, Subbarao Kambhampati
                        <br>
                        <conftitle>AAAI-22 Workshop on Reinforcement Learning in Games</conftitle>
                        <br>
                        <a class="button" href="https://arxiv.org/abs/2110.05286"><i class="fa fa-file-text"
                                                                                     aria-hidden="true"></i> paper</a>
                        <p></p>
                    </td>
                    </tr>
                    </tbody>
                </table>
                -->

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tbody>
                    <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle"><img
                            src="images/paper_imgs/2021_iros.png" style="border-radius:5%/10%;width:200px"></td>
                    <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
                        <a href="https://arxiv.org/abs/2104.00878">
                            <papertitle>
                                Contrastively Learning Visual Attention as Affordance Cues from Demonstrations
                                for Robotic Grasping
                            </papertitle>
                        </a>
                        <br>
                        Yantian Zha, Siddhant Bhambri, <u>Lin Guan</u>
                        <br>
                        <conftitle>IROS 2021</conftitle>
                        <br>
                        <a class="button" href="https://arxiv.org/abs/2104.00878"><i class="fa fa-file-text"
                                                                                     aria-hidden="true"></i> paper</a>
                        <p></p>
                    </td>
                    </tr>
                    </tbody>
                </table>

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tbody>
                    <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle"><img
                            src="images/paper_imgs/2019_atari_head.png" style="border-radius:5%/10%;width:200px"></td>
                    <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
                        <a href="https://arxiv.org/abs/1903.06754">
                            <papertitle>Atari-HEAD: Atari Human Eye-Tracking and Demonstration Dataset</papertitle>
                        </a>
                        <br>
                        Ruohan Zhang, Calen Walshe, Zhuode Liu, <u>Lin Guan</u>, Karl S. Muller, Jake A. Whritner, Luxin
                        Zhang, Mary M Hayhoe, Dana H Ballard
                        <br>
                        <conftitle>AAAI 2020</conftitle>
                        <br>
                        <a class="button" href="https://arxiv.org/abs/1903.06754"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
                        <p style="color:#4E4E4E;">
                            We provide a large-scale, high-quality dataset of human actions with simultaneously
                            recorded eye movements (i.e., gaze info) while humans play Atari video games.
                        </p>
                    </td>
                    </tr>
                    </tbody>
                </table>

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tbody>
                    <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle"><img
                            src="images/paper_imgs/2019_survey.png" style="border-radius:5%/10%;width:200px"></td>
                    <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
                        <a href="https://arxiv.org/abs/1909.09906">
                            <papertitle>Leveraging Human Guidance for Deep Reinforcement Learning Tasks</papertitle>
                        </a>
                        <br>
                        Ruohan Zhang, Faraz Torabi, <u>Lin Guan</u>, Dana H. Ballard, Peter Stone
                        <br>
                        <conftitle>IJCAI 2019, Survey Track</conftitle>
                        <br>
                        <a class="button" href="https://arxiv.org/abs/1909.09906"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
                        <p style="color:#4E4E4E;">
                            This survey provides a high-level overview of five learning frameworks that
                            primarily rely on human guidance other than conventional, step-by-step action demonstrations.
                        </p>
                    </td>
                    </tr>
                    </tbody>
                </table>
                </tbody>
            </table>
        </td>
    </tr>
    </tbody>
</table>

<table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
    <tr style="padding:0px">
        <td style="padding:0px">

            <!-- Service  -->
            <hr>
            <heading><b>Service</b></heading>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding-left:2.5%;padding-bottom:20px;width:100%;vertical-align:middle">

                        <p><ul class="fa-ul">
                            <li><i class="fa fa-chevron-right"></i>
                        <p style="display:inline;color:#ff0066;">[2022]</p> Served as a Reviewer for NeurIPS 2022.
                        </li>
                        <li><i class="fa fa-chevron-right"></i>
                            <p style="display:inline;color:#ff0066;">[2021]</p> Served as Program Committee for AAAI 2022 and ICRA 2022.
                        </li>
                        <li><i class="fa fa-chevron-right"></i>
                            <p style="display:inline;color:#ff0066;">[2020]</p> Served as Program Committee for AAAI 2021.
                        </li>
                        </ul>
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>

            <hr>
            <p align="center" style="font-size:small;color:grey;">
                website adapted from <a href="https://people.eecs.berkeley.edu/~abajcsy"
                                        style="font-size:small;">here</a>
                </font>
            </p>
        </td>
    </tr>
    </tbody>
</table>


<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-E53KV157GN"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-E53KV157GN', {
        cookie_domain: 'guansuns.github.io',
        cookie_flags: 'SameSite=None;Secure'
    });
</script>


</body>

</html>